{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlUrkC2I+LoEA6p2wul8De",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreeshambav/DeepLearning_training/blob/main/Deeplearning_Sarima_ETS_Model_STATS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "import warnings\n",
        "\n",
        "# Read the data\n",
        "file_path_merged = 'C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Working_data\\\\Budget_merged_file_Cat_with_outliers_replaced.xlsx'\n",
        "df = pd.read_excel(file_path_merged)\n",
        "\n",
        "# Drop rows with missing values in the specified columns\n",
        "df = df.dropna(subset=[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"])\n",
        "\n",
        "# Convert the values in columns \"01\" to \"12\" to numeric\n",
        "df[[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]] = df[[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Calculate the sum of values for each row\n",
        "df[\"Sum\"] = df[[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]].sum(axis=1)\n",
        "\n",
        "# Filter rows where the sum of values is greater than 0\n",
        "filtered_data = df[df[\"Sum\"] > 0]\n",
        "\n",
        "# Group the data by \"COUNTRY\" and \"Year\" and calculate the mean of the sum for each group\n",
        "grouped_data = filtered_data.groupby([\"COUNTRY\", \"Year\"])[\"Sum\"].mean()\n",
        "\n",
        "print(grouped_data)\n",
        "\n",
        "# # Group the data based on 'MRL_Category'\n",
        "# grouped_df = df.groupby('MRL_Category')\n",
        "\n",
        "\n",
        "# Group the data based on the new 'MRL_Category' columns\n",
        "grouped_df = df.groupby(['MRL_Category_Cat1', 'MRL_Category_Cat2', 'MRL_Category_Cat3', 'MRL_Category_Cat4'])\n",
        "\n",
        "# Helper function to create a concatenated MRL category string\n",
        "def get_category_string(row):\n",
        "    return f\"{row['MRL_Category_Cat1']}_{row['MRL_Category_Cat2']}_{row['MRL_Category_Cat3']}_{row['MRL_Category_Cat4']}\"\n",
        "\n",
        "# Define ARIMA model parameters\n",
        "# order = (5, 1, 0)  # (p, d, q) - ARIMA parameters\n",
        "\n",
        "# Create an empty DataFrame to store the predictions\n",
        "predictions_df = pd.DataFrame(columns=[\"Category\", \"Year\", \"Month\", \"Prediction\", \"ARIMA_Prediction\", \"SARIMA_Prediction\", \"ETS_Prediction\"])\n",
        "\n",
        "# Create an empty DataFrame to store the forecast for year 2022\n",
        "forecast_df = pd.DataFrame(columns=[\"Category\", \"Year\", \"Month\", \"ARIMA_Forecast\", \"SARIMA_Forecast\", \"ETS_Forecast\"])\n",
        "\n",
        "# Create empty lists to store statistics for ARIMA and Exponential Smoothing separately\n",
        "arima_stats_list = []\n",
        "ets_stats_list = []\n",
        "sarima_stats_list = []\n",
        "\n",
        "# Filter data for each year (2019, 2020, 2021) and apply ARIMA model for each category\n",
        "years = [2019, 2020, 2021, 2022]\n",
        "months = [\"{:02d}\".format(month) for month in range(1, 13)]\n",
        "\n",
        "# Helper function to find the last financial year\n",
        "def find_last_financial_year(years):\n",
        "    sorted_years = sorted(years, reverse=True)\n",
        "    for i in range(len(sorted_years) - 1):\n",
        "        if sorted_years[i] - 1 != sorted_years[i + 1]:\n",
        "            return sorted_years[i]\n",
        "    return sorted_years[-1]\n",
        "\n",
        "# ACF and PACF with upper and lower bound\n",
        "def plot_acf_pacf(train_data):\n",
        "    # Calculate ACF and PACF for the training data\n",
        "    acf_values, confint_acf = acf(train_data, nlags=30, alpha=0.05)\n",
        "    pacf_values, confint_pacf = pacf(train_data, nlags=30, alpha=0.05)\n",
        "\n",
        "    # Plot the ACF and PACF with upper and lower bounds\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title(\"Autocorrelation Function (ACF)\")\n",
        "    plt.stem(acf_values, markerfmt='o', linefmt='gray', basefmt='gray', use_line_collection=True)\n",
        "    plt.axhline(y=-1.96/np.sqrt(len(train_data)), linestyle='--', color='gray')  # Lower bound\n",
        "    plt.axhline(y=1.96/np.sqrt(len(train_data)), linestyle='--', color='gray')   # Upper bound\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.title(\"Partial Autocorrelation Function (PACF)\")\n",
        "    plt.stem(pacf_values, markerfmt='o', linefmt='gray', basefmt='gray', use_line_collection=True)\n",
        "    plt.axhline(y=-1.96/np.sqrt(len(train_data)), linestyle='--', color='gray')  # Lower bound\n",
        "    plt.axhline(y=1.96/np.sqrt(len(train_data)), linestyle='--', color='gray')   # Upper bound\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for category, group_df in grouped_df:\n",
        "    for year in years:\n",
        "        year_df = group_df[group_df['Year'] == year]\n",
        "\n",
        "        # Check if all 12 columns exist in the DataFrame\n",
        "        data_columns = [str(month).zfill(2) for month in range(1, 13)]\n",
        "        missing_columns = [col for col in data_columns if col not in year_df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Missing data columns for Category: {category}, Year: {year}: {missing_columns}\")\n",
        "            continue\n",
        "\n",
        "        data = year_df[data_columns].values.flatten()\n",
        "\n",
        "        # Handle cases where there might be missing data for some years\n",
        "        if np.any((data != 0) & pd.notnull(data)):\n",
        "            # Find last financial year\n",
        "            last_financial_year = find_last_financial_year(years)\n",
        "\n",
        "            # Split data into train and test sets\n",
        "            if year == last_financial_year:\n",
        "                # Last 12 months as test data\n",
        "                train_data = data[:-12]\n",
        "                test_data = data[-12:]\n",
        "            else:\n",
        "                # Rest as train data\n",
        "                train_data = data[:-12]  # Use all available data except the last 12 months\n",
        "                test_data = data[-12:]  # Use the last 12 months as test data\n",
        "\n",
        "            # Perform ADF test to check for stationarity\n",
        "            adf_result = adfuller(train_data)\n",
        "            p_value = adf_result[1]\n",
        "            is_stationary = p_value < 0.05\n",
        "            print(f\"Category: {category}, Year: {year} - Time series is Stationary.\")\n",
        "\n",
        "            if not is_stationary:\n",
        "                print(f\"Category: {category}, Year: {year} - Time series is non-stationary.\")\n",
        "                continue\n",
        "\n",
        "            # Try different ARIMA model orders and methods\n",
        "            orders_to_try = [(5, 1, 0), (4, 1, 0), (3, 1, 0)]  # Add more orders to try\n",
        "\n",
        "            model_fit = None\n",
        "            converged = False\n",
        "            for order in orders_to_try:\n",
        "                try:\n",
        "                    # Try fitting ARIMA model using different orders\n",
        "                    with warnings.catch_warnings():\n",
        "                        warnings.simplefilter(\"ignore\")  # Ignore ConvergenceWarning\n",
        "                        model = ARIMA(train_data, order=order)\n",
        "                        model_fit = model.fit()\n",
        "                        arima_predictions = model_fit.forecast(steps=12)\n",
        "                    converged = True\n",
        "                    break  # Break if the model successfully converges\n",
        "                except Exception as e:\n",
        "                    print(f\"ARIMA model did not converge for Category: {category}, Year: {year}, Order: {order}\")\n",
        "                    print(f\"Error message: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if not converged:\n",
        "                print(f\"ARIMA model did not converge for any combination of orders for Category: {category}, Year: {year}.\")\n",
        "                continue\n",
        "\n",
        "            # Flatten the arima_predictions to match the shape of test_data\n",
        "            arima_predictions = arima_predictions.flatten()\n",
        "\n",
        "            # Fit exponential smoothing model and make predictions for the out-of-sample period (2022)\n",
        "            ets_model = ExponentialSmoothing(train_data, seasonal='add', seasonal_periods=12)\n",
        "            ets_fit = ets_model.fit()\n",
        "            ets_predictions = ets_fit.forecast(steps=12)\n",
        "            # Flatten the ets_predictions to match the shape of test_data\n",
        "            ets_predictions = ets_predictions.flatten()\n",
        "\n",
        "            # Fit SARIMA model and make predictions for the out-of-sample period (2022)\n",
        "            sarima_model = SARIMAX(train_data, order=order, seasonal_order=(0, 1, 1, 12))\n",
        "            sarima_fit = sarima_model.fit()\n",
        "            sarima_predictions = sarima_fit.forecast(steps=12)\n",
        "            # Flatten the sarima_predictions to match the shape of test_data\n",
        "            sarima_predictions = sarima_predictions.flatten()\n",
        "\n",
        "            # ACF and PACF plots\n",
        "            plot_acf_pacf(train_data)\n",
        "\n",
        "            # Calculate Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE)\n",
        "            if np.any(test_data):\n",
        "                mse_arima = mean_squared_error(test_data.reshape(-1, 1), arima_predictions.reshape(-1, 1))\n",
        "                rmse_arima = np.sqrt(mse_arima)\n",
        "                z_scores_test_arima = (test_data - np.mean(test_data)) / np.std(test_data)\n",
        "                z_scores_pred_arima = (arima_predictions - np.mean(arima_predictions)) / np.std(arima_predictions)\n",
        "\n",
        "                # Calculate MAPE only for non-zero elements in test_data\n",
        "                non_zero_indices_arima = test_data != 0\n",
        "                mape_arima = np.mean(np.abs((test_data[non_zero_indices_arima] - arima_predictions[non_zero_indices_arima]) / test_data[non_zero_indices_arima])) * 100\n",
        "\n",
        "                print(\"ARIMA - MSE:\", mse_arima, \"RMSE:\", rmse_arima, \"MAPE:\", mape_arima, \"Z_score:\", z_scores_test_arima, \"Z_scores:\", z_scores_pred_arima)\n",
        "\n",
        "                mse_ets = mean_squared_error(test_data.reshape(-1, 1), ets_predictions.reshape(-1, 1))\n",
        "                rmse_ets = np.sqrt(mse_ets)\n",
        "                z_scores_test_ets = (test_data - np.mean(test_data)) / np.std(test_data)\n",
        "                z_scores_pred_ets = (ets_predictions - np.mean(ets_predictions)) / np.std(ets_predictions)\n",
        "\n",
        "                # Calculate MAPE only for non-zero elements in test_data\n",
        "                non_zero_indices_ets = test_data != 0\n",
        "                mape_ets = np.mean(np.abs((test_data[non_zero_indices_ets] - ets_predictions[non_zero_indices_ets]) / test_data[non_zero_indices_ets])) * 100\n",
        "\n",
        "                print(\"Exponential Smoothing - MSE:\", mse_ets, \"RMSE:\", rmse_ets, \"MAPE:\", mape_ets, \"Z_score:\", z_scores_test_ets, \"Z_scores:\", z_scores_pred_ets)\n",
        "\n",
        "                mse_sarima = mean_squared_error(test_data.reshape(-1, 1), sarima_predictions.reshape(-1, 1))\n",
        "                rmse_sarima = np.sqrt(mse_sarima)\n",
        "                z_scores_test_sarima = (test_data - np.mean(test_data)) / np.std(test_data)\n",
        "                z_scores_pred_sarima = (sarima_predictions - np.mean(sarima_predictions)) / np.std(sarima_predictions)\n",
        "\n",
        "                # Calculate MAPE only for non-zero elements in test_data\n",
        "                non_zero_indices_sarima = test_data != 0\n",
        "                mape_sarima = np.mean(np.abs((test_data[non_zero_indices_sarima] - sarima_predictions[non_zero_indices_sarima]) / test_data[non_zero_indices_sarima])) * 100\n",
        "\n",
        "                print(\"SARIMA - MSE:\", mse_sarima, \"RMSE:\", rmse_sarima, \"MAPE:\", mape_sarima, \"Z_score:\", z_scores_test_sarima, \"Z_scores:\", z_scores_pred_sarima)\n",
        "            else:\n",
        "                print(f\"No test data available for Category: {category}, Year: {year}\")\n",
        "\n",
        "            # Append the statistics values to the appropriate lists\n",
        "            arima_stats_list.append({\"Category\": category, \"Year\": year, \"Model\": \"ARIMA\", \"MSE\": mse_arima, \"RMSE\": rmse_arima, \"MAPE\": mape_arima, \"z_scores_t\": z_scores_test_arima, \"z_scores_p\": z_scores_pred_arima})\n",
        "            ets_stats_list.append({\"Category\": category, \"Year\": year, \"Model\": \"Exponential Smoothing\", \"MSE\": mse_ets, \"RMSE\": rmse_ets, \"MAPE\": mape_ets, \"z_scores_t\": z_scores_test_ets, \"z_scores_p\": z_scores_pred_ets})\n",
        "            sarima_stats_list.append({\"Category\": category, \"Year\": year, \"Model\": \"SARIMA\", \"MSE\": mse_sarima, \"RMSE\": rmse_sarima, \"MAPE\": mape_sarima, \"z_scores_t\": z_scores_test_sarima, \"z_scores_p\": z_scores_pred_sarima})\n",
        "\n",
        "            # Store the predictions and forecasts in the respective DataFrames\n",
        "            for i in range(12):\n",
        "                month = months[i]\n",
        "                prediction_row = {\"Category\": category, \"Year\": year, \"Month\": month,\n",
        "                                  \"Prediction\": test_data[i], \"ARIMA_Prediction\": arima_predictions[i],\n",
        "                                  \"SARIMA_Prediction\": sarima_predictions[i], \"ETS_Prediction\": ets_predictions[i]}\n",
        "                predictions_df = predictions_df.append(prediction_row, ignore_index=True)\n",
        "\n",
        "            # Forecast for year 2022 (for all 12 months)\n",
        "            if year == last_financial_year:\n",
        "                for i in range(12):\n",
        "                    month = months[i]\n",
        "                    forecast_row = {\"Category\": category, \"Year\": 2022, \"Month\": month,\n",
        "                                    \"ARIMA_Forecast\": arima_predictions[i], \"SARIMA_Forecast\": sarima_predictions[i], \"ETS_Forecast\": ets_predictions[i] }\n",
        "                    forecast_df = forecast_df.append(forecast_row, ignore_index=True)\n",
        "\n",
        "# Save the predictions and forecast DataFrames to CSV\n",
        "predictions_df.to_csv(\"C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Sarima\\\\predictions.csv\", index=False)\n",
        "forecast_df.to_csv(\"C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Sarima\\\\forecast.csv\", index=False)\n",
        "\n",
        "# Save the statistics DataFrames to CSV\n",
        "arima_stats_df = pd.DataFrame(arima_stats_list)\n",
        "ets_stats_df = pd.DataFrame(ets_stats_list)\n",
        "sarima_stats_df = pd.DataFrame(sarima_stats_list)\n",
        "arima_stats_df.to_csv(\"C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Sarima\\\\arima_stats.csv\", index=False)\n",
        "ets_stats_df.to_csv(\"C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Sarima\\\\ets_stats.csv\", index=False)\n",
        "sarima_stats_df.to_csv(\"C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Sarima\\\\sarima_stats.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Z8YjwEiLcidN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}