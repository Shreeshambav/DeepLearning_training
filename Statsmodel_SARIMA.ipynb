{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFvq1+bXbxzxCPIR1KfS4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreeshambav/DeepLearning_training/blob/main/Statsmodel_SARIMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOmqbGvj2KWL"
      },
      "outputs": [],
      "source": [
        "# Test 7 - SARIMA - Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "import warnings\n",
        "\n",
        "# Read the data\n",
        "file_path_merged = 'C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Budget_merged_file_Cat.xlsx'\n",
        "df = pd.read_excel(file_path_merged)\n",
        "\n",
        "# Replacing NAN with different categories\n",
        "df['VISION_SBU'].fillna('Missing', inplace=True)\n",
        "df['ACCOUNT_OFFICER'].fillna('0000', inplace=True)\n",
        "\n",
        "# Drop rows with missing values in the specified columns\n",
        "df = df.dropna(subset=[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"])\n",
        "\n",
        "# Convert the values in columns \"01\" to \"12\" to numeric\n",
        "df[[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]] = df[[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Calculate the sum of values for each row\n",
        "df[\"Sum\"] = df[[\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]].sum(axis=1)\n",
        "\n",
        "# Filter rows where the sum of values is greater than 0\n",
        "filtered_data = df[df[\"Sum\"] > 0]\n",
        "\n",
        "# Group the data by \"COUNTRY\" and \"Year\" and calculate the mean of the sum for each group\n",
        "grouped_data = filtered_data.groupby([\"COUNTRY\", \"Year\"])[\"Sum\"].mean()\n",
        "\n",
        "print(grouped_data)\n",
        "\n",
        "# Group the data based on 'MRL_Category'\n",
        "grouped_df = df.groupby('MRL_Category')\n",
        "\n",
        "# Create an empty DataFrame to store the predictions\n",
        "predictions_df = pd.DataFrame(columns=[\"Category\", \"Year\", \"Month\", \"Prediction\", \"Forecast Trend\"])\n",
        "\n",
        "# Create empty lists to store statistics for ARIMA and Exponential Smoothing separately\n",
        "arima_stats_list = []\n",
        "ets_stats_list = []\n",
        "\n",
        "# Filter data for each year (2019, 2020, 2021) and apply ARIMA model for each category\n",
        "years = [2019, 2020, 2021, 2022]\n",
        "months = [\"{:02d}\".format(month) for month in range(1, 13)]\n",
        "\n",
        "# Helper function to find the last financial year\n",
        "def find_last_financial_year(years):\n",
        "    sorted_years = sorted(years, reverse=True)\n",
        "    for i in range(len(sorted_years) - 1):\n",
        "        if sorted_years[i] - 1 != sorted_years[i + 1]:\n",
        "            return sorted_years[i]\n",
        "    return sorted_years[-1]\n",
        "\n",
        "for category, group_df in grouped_df:\n",
        "    for year in years:\n",
        "        year_df = group_df[group_df['Year'] == year]\n",
        "        data_columns = [str(month).zfill(2) for month in range(1, 13)]\n",
        "        missing_columns = [col for col in data_columns if col not in year_df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"Missing data columns for Category: {category}, Year: {year}: {missing_columns}\")\n",
        "            continue\n",
        "\n",
        "        data = year_df[data_columns].values.flatten()\n",
        "\n",
        "        # Handle cases where there might be missing data for some years\n",
        "        if np.any((data != 0) & pd.notnull(data)):\n",
        "            last_financial_year = find_last_financial_year(years)\n",
        "\n",
        "            # Split data into train and test sets\n",
        "            if year == last_financial_year:\n",
        "                # Last 12 months as test data\n",
        "                train_data = data[:-12]\n",
        "                test_data = data[-12:]\n",
        "            else:\n",
        "                # Rest as train data\n",
        "                train_data = data[:-12]\n",
        "                test_data = data[-12:]\n",
        "\n",
        "            # Perform ADF test to check for stationarity\n",
        "            adf_result = adfuller(train_data)\n",
        "            p_value = adf_result[1]\n",
        "            is_stationary = p_value < 0.05\n",
        "            print(f\"Category: {category}, Year: {year} - Time series is Stationary.\")\n",
        "\n",
        "            if not is_stationary:\n",
        "                print(f\"Category: {category}, Year: {year} - Time series is non-stationary.\")\n",
        "                continue\n",
        "\n",
        "            # Try different ARIMA model orders and methods\n",
        "            orders_to_try = [(5, 1, 0), (4, 1, 0), (3, 1, 0)]\n",
        "\n",
        "            model_fit = None\n",
        "            converged = False\n",
        "            for order in orders_to_try:\n",
        "                try:\n",
        "                    with warnings.catch_warnings():\n",
        "                        warnings.simplefilter(\"ignore\")  # Ignore ConvergenceWarning\n",
        "                        model = ARIMA(train_data, order=order)\n",
        "                        model_fit = model.fit()\n",
        "                        arima_predictions = model_fit.forecast(steps=12)\n",
        "                    converged = True\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"ARIMA model did not converge for Category: {category}, Year: {year}, Order: {order}\")\n",
        "                    print(f\"Error message: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if not converged:\n",
        "                print(f\"ARIMA model did not converge for any combination of orders for Category: {category}, Year: {year}.\")\n",
        "                continue\n",
        "\n",
        "            arima_predictions = arima_predictions.flatten()\n",
        "\n",
        "            # Fit SARIMA model and make predictions for the out-of-sample period (2022)\n",
        "            order = (5, 1, 0)\n",
        "            seasonal_order = (0, 1, 1, 12)\n",
        "            sarima_model = SARIMAX(train_data, order=order, seasonal_order=seasonal_order)\n",
        "            sarima_fit = sarima_model.fit(disp=False)\n",
        "            sarima_predictions = sarima_fit.forecast(steps=12)\n",
        "            sarima_predictions = sarima_predictions.flatten()\n",
        "\n",
        "##################################################################################################\n",
        "\n",
        "            # Fit exponential smoothing model and make predictions for the out-of-sample period (2022)\n",
        "            ets_model = ExponentialSmoothing(train_data, seasonal='add', seasonal_periods=12)\n",
        "            ets_fit = ets_model.fit()\n",
        "            ets_predictions = ets_fit.forecast(steps=12)\n",
        "            # Flatten the ets_predictions to match the shape of test_data\n",
        "            ets_predictions = ets_predictions.flatten()\n",
        "\n",
        "###################################################################################################\n",
        "\n",
        "            # Calculate Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE)\n",
        "            if np.any(test_data):\n",
        "                mse_arima = mean_squared_error(test_data.reshape(-1, 1), arima_predictions.reshape(-1, 1))\n",
        "                rmse_arima = np.sqrt(mse_arima)\n",
        "                z_scores_test_arima = (test_data - np.mean(test_data)) / np.std(test_data)\n",
        "                z_scores_pred_arima = (arima_predictions - np.mean(arima_predictions)) / np.std(arima_predictions)\n",
        "\n",
        "                non_zero_indices_arima = test_data != 0\n",
        "                mape_arima = np.mean(np.abs((test_data[non_zero_indices_arima] - arima_predictions[non_zero_indices_arima]) / test_data[non_zero_indices_arima])) * 100\n",
        "\n",
        "                mse_sarima = mean_squared_error(test_data.reshape(-1, 1), sarima_predictions.reshape(-1, 1))\n",
        "                rmse_sarima = np.sqrt(mse_sarima)\n",
        "                z_scores_test_sarima = (test_data - np.mean(test_data)) / np.std(test_data)\n",
        "                z_scores_pred_sarima = (sarima_predictions - np.mean(sarima_predictions)) / np.std(sarima_predictions)\n",
        "\n",
        "                non_zero_indices_sarima = test_data != 0\n",
        "                mape_sarima = np.mean(np.abs((test_data[non_zero_indices_sarima] - sarima_predictions[non_zero_indices_sarima]) / test_data[non_zero_indices_sarima])) * 100\n",
        "\n",
        "                arima_stats_list.append({\"Category\": category, \"Year\": year, \"Model\": \"ARIMA\", \"MSE\": mse_arima, \"RMSE\": rmse_arima, \"MAPE\": mape_arima, \"z_scores_t\": z_scores_test_arima, \"z_scores_p\": z_scores_pred_arima})\n",
        "                ets_stats_list.append({\"Category\": category, \"Year\": year, \"Model\": \"SARIMA\", \"MSE\": mse_sarima, \"RMSE\": rmse_sarima, \"MAPE\": mape_sarima, \"z_scores_t\": z_scores_test_sarima, \"z_scores_p\": z_scores_pred_sarima})\n",
        "\n",
        "                print(f\"Category: {category}, Year: {year}\")\n",
        "                print(\"ARIMA - MSE:\", mse_arima, \"RMSE:\", rmse_arima, \"MAPE:\", mape_arima, \"z_scores_t:\", z_scores_test_arima, \"z_scores_p:\", z_scores_pred_arima)\n",
        "                print(\"SARIMA - MSE:\", mse_sarima, \"RMSE:\", rmse_sarima, \"MAPE:\", mape_sarima, \"z_scores_t:\", z_scores_test_sarima, \"z_scores_p:\", z_scores_pred_sarima)\n",
        "                print()\n",
        "            else:\n",
        "                print(f\"No test data available for Category: {category}, Year: {year}\")\n",
        "\n",
        "                # Append the predictions and forecast trend to the DataFrame\n",
        "                forecast_trend = np.append(train_data[-1], arima_predictions.cumsum())\n",
        "                prediction_data = []\n",
        "                for month, arima_pred, sarima_pred, ets_pred, trend_value in zip(months[-12:], arima_predictions, sarima_predictions, ets_predictions, forecast_trend[-12:]):\n",
        "                    prediction_data.append({\"Category\": category, \"Year\": year, \"Month\": month, \"Prediction\": arima_pred, \"ARIMA_Prediction\": arima_pred, \"SARIMA_Prediction\": sarima_pred, \"ETS_Prediction\": ets_pred, \"Forecast Trend\": trend_value})\n",
        "\n",
        "                predictions_df = pd.concat([predictions_df, pd.DataFrame(prediction_data)], ignore_index=True)\n",
        "                # Plot the actual data and predictions\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                # Create a time range spanning from the beginning of the train data to the end of the prediction period (2022)\n",
        "                time_range = np.arange(1, len(train_data) + len(arima_predictions) + 1)\n",
        "\n",
        "                # Plot actual data\n",
        "                plt.plot(time_range[:len(train_data)], train_data, label=\"Train Data\", color=\"gray\")\n",
        "                plt.plot(time_range[-12:], test_data, label=\"Test Data\", color=\"black\")\n",
        "\n",
        "                # Plot ARIMA predictions\n",
        "                pred_time_range = np.arange(len(train_data), len(train_data) + len(arima_predictions))\n",
        "                plt.plot(pred_time_range, arima_predictions, label=\"ARIMA Predictions\", linestyle=\"--\", color=\"blue\")\n",
        "\n",
        "                # Plot SARIMA predictions\n",
        "                plt.plot(pred_time_range, sarima_predictions, label=\"SARIMA Predictions\", linestyle=\"--\", color=\"green\")\n",
        "\n",
        "                # Plot exponential smoothing predictions\n",
        "                plt.plot(pred_time_range, ets_predictions, label=\"Exponential Smoothing Predictions\")\n",
        "                # Plot the trend of the forecast for 2022 (prediction)\n",
        "\n",
        "                forecast_time_range = np.arange(len(train_data), len(train_data) + len(arima_predictions) + 1)\n",
        "                plt.plot(forecast_time_range, forecast_trend, label=\"Forecast Trend\", linestyle=\"--\", color=\"orange\")\n",
        "\n",
        "                plt.xlabel(\"Months\")\n",
        "                plt.ylabel(\"Values\")\n",
        "                plt.title(f\"Category: {category}, Year: {year}\")\n",
        "                plt.legend(loc=\"upper left\")\n",
        "                plt.grid(True)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "################################################################################\n",
        "# Create DataFrames from the lists of statistics\n",
        "arima_stats_df = pd.DataFrame(arima_stats_list)\n",
        "ets_stats_df = pd.DataFrame(ets_stats_list)\n",
        "\n",
        "# Concatenate ARIMA and SARIMA statistics DataFrames\n",
        "stats_df = pd.concat([arima_stats_df, ets_stats_df], ignore_index=True)\n",
        "\n",
        "# Save the pred_output_file DataFrame to an Excel file\n",
        "pred_output_file = 'C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Budget_merged_file_Cat_predicted.xlsx'\n",
        "predictions_df.to_excel(pred_output_file, index=False)\n",
        "\n",
        "# Save the statistics DataFrame to an Excel file\n",
        "stats_output_file = 'C:\\\\Users\\\\mraj4\\\\Documents\\\\OptimumPython\\\\DS\\\\DL\\\\Sunoida\\\\Budget_merged_file_Cat_predicted_Stats.xlsx'\n",
        "stats_df.to_excel(stats_output_file, index=False)"
      ]
    }
  ]
}